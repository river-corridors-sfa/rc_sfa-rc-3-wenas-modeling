---
title: "01_Wenas_meta"
output: html_document
date: "2023-09-06"
editor_options: 
  chunk_output_type: console
---

The purpose of this script is take the output from SCOPUS and Web of Science and merge them into 1 working document. 

## Load packages and set working directory
```{r Jake/Mac}
#for Jake/mac

rm(list=ls(all=T)) #this clears your Environment

library(readr)
library(tidyverse)
library(here)
library(readxl)
library(ggmap)
library(sf)

```


```{r merge web of science and Scopus results}
web_meta <- read_csv(here("inputs", "Metaanalysis_webofscience.csv"), skip = 10) # reading in web of science output

web_meta <-web_meta %>% 
  select(-("1900":"2023")) %>% 
  mutate_all(as.character) # removing columns that are not necessary. Column 1900:2023 are purely counting the amount of citations per year and I don't care for that. I am also making all columns characters so it will be easier to merge. 

scopus_meta <- read_csv(here("inputs", "Wenas_meta_analysis_scopus.csv")) # reading in the scopus output

scopus_meta <- scopus_meta %>% 
  mutate_all(as.character) # Changing all columns to characters so it will be easier to merge

merged_meta <- full_join(web_meta, scopus_meta) %>% 
  distinct(DOI, .keep_all = TRUE) # merging both dataframes and removing duplicated DOIs. This yields 319 studies. 

# export dataframe 
write_csv(merged_meta, here("Output_for_analysis", "merged_meta.csv")) # exporting merged dataframe into outputs file


```

```{r round 2 of filter, plot where the 1st round filter on a map to see if we should filter by geographic location}
library("rnaturalearth")
library("rnaturalearthdata")

meta <- read_excel(here("inputs", "StudiesData_Table1.xlsx"), 
    sheet = "Study_info_filtered_take_one") # reading in filtered meta data sheet

coords <- meta %>% 
  select(Lat, Long) %>% 
  mutate(across(everything(), as.numeric)) # creating dataframe that is just the coordinates

world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)

ggplot(data = world) +
    geom_sf() +
    geom_point(data = coords, aes(x = Long, y = Lat), size = 4, 
        shape = 23, fill = "darkred") 

ggsave("meta_map_filter_1.pdf",
       path = here("initial_plots", "01_Wenas_meta_wrangle"),
       width = 8, height = 10, units = "in")

# We decided to remove all the studies that were out of the country

```


```{r data prepper}
studies_file_list <- list.files(path = "inputs/Studies/meta_final/", 
                                  recursive=F, 
                                  pattern=".csv", 
                                  full.names=TRUE)

storm_list_beta<-do.call("list", lapply(studies_file_list, 
                                        read.csv, 
                                        stringsAsFactors=FALSE, 
                                        header=T))

meta_df <-do.call("rbind", lapply(studies_file_list, 
                                     read.csv, 
                                     check.names = FALSE,
                                     stringsAsFactors=FALSE, 
                                     header=T, blank.lines.skip = TRUE, fill=TRUE)) # Should be 6033


unitconv=read.csv("inputs/UnitConversion.csv",stringsAsFactors = FALSE)

# checking to see if my manual input of the meta analysis is good or not. 
meta_df %>% 
  group_by(`Design (Control/Burn; Pre/Post)`, Study_ID) %>% 
  n_distinct(meta_df$Study_ID)

mean <- meta_df %>% 
  filter(Mean_Median_or_IndividualSample == "Mean")
table(mean$Study_ID)

unique(meta_df$DOC_unit)
unique(meta_df$NO3_unit)

unique(meta_df$`Design (Control/Burn; Pre/Post)`)
unique(meta_df$Study_ID)

table(mean$Time_Since_Fire)

```











